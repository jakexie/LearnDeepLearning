最初的深度神经网络不包含卷积层（例如 多层感知网络 MLP）：

Dense

Activation(‘relu’)

Dropout

Dense

Activation(‘relu’)

Dropout

Dense

Activation(‘softMax’)

SoftMax 作用: 归一化输入值（Wx + b）到（0，1），即转换成每种类别的概率，起到分类器的作用

\begin{eqnarray}
  \sum_j a^L_j & = & \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.
\tag{79}\end{eqnarray}
使用e指数时因为保证所有结果为正数

SoftMax 输出结果总和为1，即反正输入数据是多个类别的概率分布
挑选最大的为最终分类标签（标签 有0,1,2,…,n）


==》注意和sigmoid 区别，sigmoid 一般用于2分类，将某一数值归一化到（0,1）
挑选>某个阈值确定最终分类标签（标签只有0,1）

所谓全连接直观理解就是 神经元同上一层的神经元都有连接，即通过矩阵点乘获得 netron(i) = W(row(i))*X (X为上一层所有神经元展开成的列向量)
全连接参数过多，容易过拟合

经过卷积操作后的维度计算：
N = (W -F +2P)/S +1
W 图像宽 F 卷积核宽 P padding数

1998 letnet5
使用机器学习特征的原因--》 
    1. 计算性能的提升及计算单元日益廉价 
    2. 大量真实数据的获取越来越方便
    3. 能处理高维大量数据并做出复杂决策的机器学习技术的出现
第三层卷积C3 10×10×16 由上一层S2 14×14×6 生成，且第三层16个特征图并没有同第2层6个特征图全部连接，详细连接情况见论文表格：
    1-6层用了连续 3层特征
    7-12层用了连续 4层特征
    12-15层用了 不连续 4层特征
    16 层用了 6层特征 

2012 alexnet
训练数据纬度：256×256 --> 224×224
数据处理：
    数据处理的目的增加训练数据量，减少过拟合
    1.水平转换
    2.随机分割
    这两种操作大大增加了数据量（2048倍 2*(256-224)*(256-224)）
    3.修改图片RGB三通道强度(即改变光照和强度)


