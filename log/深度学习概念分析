最初的深度神经网络不包含卷积层（例如 多层感知网络 MLP）：

Dense

Activation(‘relu’)

Dropout

Dense

Activation(‘relu’)

Dropout

Dense

Activation(‘softMax’)

SoftMax 作用: 归一化输入值（Wx + b）到（0，1），即转换成每种类别的概率，起到分类器的作用

\begin{eqnarray}
  \sum_j a^L_j & = & \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.
\tag{79}\end{eqnarray}
使用e指数时因为保证所有结果为正数

SoftMax 输出结果总和为1，即反正输入数据是多个类别的概率分布
挑选最大的为最终分类标签（标签 有0,1,2,…,n）


==》注意和sigmoid 区别，sigmoid 一般用于2分类，将某一数值归一化到（0,1）
挑选>某个阈值确定最终分类标签（标签只有0,1）

所谓全连接直观理解就是 神经元同上一层的神经元都有连接，即通过矩阵点乘获得 netron(i) = W(row(i))*X (X为上一层所有神经元展开成的列向量)
全连接参数过多，容易过拟合

经过卷积操作后的维度计算：
N = (W -F +2P)/S +1
W 图像宽 F 卷积核宽 P padding数

反卷积操作后维度计算：
N = (W - 1) * S + F - 2 * P;

1998 letnet5
使用机器学习特征的原因--》 
    1. 计算性能的提升及计算单元日益廉价 
    2. 大量真实数据的获取越来越方便
    3. 能处理高维大量数据并做出复杂决策的机器学习技术的出现
第三层卷积C3 10×10×16 由上一层S2 14×14×6 生成，且第三层16个特征图并没有同第2层6个特征图全部连接，详细连接情况见论文表格：
    1-6层用了连续 3层特征
    7-12层用了连续 4层特征
    12-15层用了 不连续 4层特征
    16 层用了 6层特征 

2012 alexnet
训练数据纬度：256×256 --> 224×224
数据处理：
    数据处理的目的增加训练数据量，减少过拟合
    1.水平转换
    2.随机分割
    这两种操作大大增加了数据量（2048倍 2*(256-224)*(256-224)）
    3.修改图片RGB三通道强度(即改变光照和强度)

2015 resnet
identity mapping(恒等映射结构): 
    以res50 为例：
        input 
        step1: branch_a conv_1(1*1*256) -> bn_1 -> scale_1
        step2: branch_b conv_1(1*1*64) -> bn_1 -> scale_1 -> relu_1 
                     -> conv_2(3*3*64) -> bn_2 -> scale_2 -> relu_2
                     -> conv_3(1*1*256) -> bn_3 -> scale_3
        step3: branch_a + branch_b -> relu

1997 LSTM
LSTM本质上是利用了数据的序列特性（例如时间上的前后相关性），而普通的dnn是不知道数据的序列特性的。
Embedding 层： 降维聚类
    Embedding 层输入一般为一句话（语句中每个单词已转换为one hot），输出为单词中每个单词对应的vector（即对应的编码）
LSTM 参数计算： （m + n + 1）*n
    m+n: 每个LSTM cell 的输入为当前输入xt和上一个cell的输出ht-1的串联 concatenate(xt+ht-1), xt为m维向量, ht-1为n维向量
    1：偏置
    n：LSTM cell的输出维度
    因此 权重矩阵的参数个数 为 输入维度（m+n）+ 偏置（1） × 输出维度（n）
