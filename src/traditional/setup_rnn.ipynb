{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"/home/super-workstation/program/LearnDeepLearning/data/imdb.npz\",\n",
    "                                                      num_words=10000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "word_index = imdb.get_word_index(path=\"imdb_word_index.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxl = 80\n",
    "'''\n",
    "for s in x_train:\n",
    "    maxl = max(len(s), maxl)\n",
    "for s in x_test:\n",
    "    maxl = max(len(s), maxl)\n",
    "'''\n",
    "\n",
    "# 统一输入长度 input_length(输入为一维数据)\n",
    "pad_train = pad_sequences(x_train, maxlen=maxl, padding='pre')\n",
    "pad_test = pad_sequences(x_test, maxlen=maxl, padding='pre')\n",
    "print(pad_train[0])\n",
    "print(pad_test[0])\n",
    "print(maxl)\n",
    "print(pad_train.shape)\n",
    "print(pad_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup net\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Flatten, GlobalAveragePooling1D, LSTM\n",
    "\n",
    "voc_size = 20000\n",
    "embedding_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_size, input_length=maxl, name='layer_x'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(LSTM(100, name='layer_lstm'))\n",
    "#model.add(AveragePooling1D(pool_size=maxl))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('layer_lstm').output)\n",
    "intermediate_output = intermediate_layer_model.predict(pad_test)\n",
    "print(intermediate_output.shape)\n",
    "print(intermediate_output[0][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate\n",
    "import sys\n",
    "#print(sys.path)\n",
    "sys.path.insert(0, '../')\n",
    "from dpl import utils\n",
    "utils.trainAndEvaluateData(model, pad_train, y_train, pad_test, y_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import numpy as np\n",
    "test_data_c = ['a little confused as to how this movie is so extolled worldwide.', 'Basically there\\'s a family where a little boy \\\n",
    "(Jake) thinks there\\'s a zombie in his closet & his parents are fighting all the time. \\\n",
    "This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie. \\\n",
    "OK, first of all when you\\'re going to make a film you must Decide if its a thriller or a drama! \\\n",
    "As a drama the movie is watchable. Parents are divorcing & arguing like in real life. \\\n",
    "And then we have Jake with his closet which totally ruins all the film! \\\n",
    "I expected to see a BOOGEYMAN similar movie, and instead i watched a \\\n",
    "drama with some meaningless thriller spots.\\\n",
    "3 out of 10 just for the well playing parents & descent dialogs. As for\\\n",
    "the shots with Jake: just ignore them',\n",
    "'This movie is too over rated. I think it suite for 5/10. Its boring \\\n",
    "and had a bad ending for a 2.5 hours Movie. I think it better for you \\\n",
    "to just watch from the review of other people.', \n",
    "'terrible bad','A very boring movie, waited for so long for the movie to end quickly. I was expecting something more at \\\n",
    "the start of the movie, but it ends up getting more and more unwanted to watch.',\n",
    "'''This show was an amazing, fresh & innovative idea in the 70\\'s when it first aired. \\\n",
    "The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was \\\n",
    "not really funny anymore, and it\\'s continued its decline further to the complete waste of time\\\n",
    "it is today. It\\'s truly disgraceful how far this show has fallen. The writing is painfully bad \\ \n",
    "the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, \\\n",
    "this show probably wouldn\\'t still be on the air. I find it so hard to believe that the same creator \\ \n",
    "that hand-selected the original cast also chose the band of hacks that followed. How can one recognize \\\n",
    "such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of \\\n",
    "respect for the original cast that made this show such a huge success. As it is now, the show is just awful. \\\n",
    "I can\\'t believe it\\'s still on the air.''']\n",
    "test_label =[1, 0, 1, 0]\n",
    "print(type(x_train[0]))\n",
    "test_data = np.zeros([len(test_data_c)], dtype=object)\n",
    "print(test_data.shape)\n",
    "row_count = 0\n",
    "for re in test_data_c:\n",
    "    splits = re.split()\n",
    "    len_s = len(splits)\n",
    "    count = 0\n",
    "    test_data[row_count] = []\n",
    "    for char_s in splits:\n",
    "        str = '0'\n",
    "        if char_s in word_index:\n",
    "            str = word_index[char_s]\n",
    "        \n",
    "        test_data[row_count].append(str)    \n",
    "        #print(row[0][count])\n",
    "        count+=1\n",
    "    row_count+=1\n",
    "pad_test = pad_sequences(test_data, maxlen=maxl, padding='pre')\n",
    "results = model.predict(pad_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"/home/super-workstation/program/LearnDeepLearning/data/reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "import os\n",
    "print(os.path.abspath('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
