{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/super-workstation/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#load data\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data(\"/home/super-workstation/program/LearnDeepLearning/data/mnist.npz\")\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 09:48:38.078193 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0812 09:48:38.163241 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 09:48:38.252479 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0812 09:48:38.269338 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0812 09:48:38.280289 140243120809792 deprecation.py:506] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0812 09:48:38.311900 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 09:48:38.322460 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n",
      "(None, 1024)\n",
      "(None, 10)\n",
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel.fit(x_train, y_train, epochs=5)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup shallow network 一个隐藏层\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28))) # 展开\n",
    "print(model.output_shape)\n",
    "model.add(Dense(1024, activation='relu')) # 全连接(隐藏层1)\n",
    "print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) # 输出\n",
    "print(model.output_shape)\n",
    "\n",
    "sgd=SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"]) # 编译\n",
    "\n",
    "# train model\n",
    "print(x_train.shape)\n",
    "'''\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredicts = model.predict(x_test)\\n\\nplt.figure(figsize=(10,10))\\nfor i in range(49):\\n    plt.subplot(7,7,i+1)\\n    plt.xticks([])\\n    plt.yticks([])\\n    plt.grid(False)\\n    #plt.imshow(x_test[i])#, cmap=plt.cm.binary)\\n    pred = predicts[i].tolist()\\n    pred_num = pred.index(max(pred))\\n    plt.xlabel(pred_num)\\n    plt.ylabel(y_test[i])\\n    if pred_num != y_test[i]:\\n        plt.imshow(x_test[i], cmap=plt.cm.binary)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "predicts = model.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(49):\n",
    "    plt.subplot(7,7,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    #plt.imshow(x_test[i])#, cmap=plt.cm.binary)\n",
    "    pred = predicts[i].tolist()\n",
    "    pred_num = pred.index(max(pred))\n",
    "    plt.xlabel(pred_num)\n",
    "    plt.ylabel(y_test[i])\n",
    "    if pred_num != y_test[i]:\n",
    "        plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 09:48:51.290806 140243120809792 deprecation_wrapper.py:119] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1-->  (None, 28, 28, 6)\n",
      "avg_pooling_1-->  (None, 14, 14, 6)\n",
      "conv2d_2-->  (None, 10, 10, 16)\n",
      "avg_pooling_2-->  (None, 5, 5, 16)\n",
      "flatten -->  (None, 400)\n",
      "full_1 -->  (None, 120)\n",
      "full_2 -->  (None, 84)\n",
      "output -->  (None, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# train\\nimport datetime\\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\\n# train\\nstart_tm = datetime.datetime.now()\\nmodel.fit(x_train, y_train, epochs=5)\\nend_tm = datetime.datetime.now()\\nprint(\"cost: \", (end_tm-start_tm).seconds)\\n\\n# evaluate\\nscores = model.evaluate(x_test, y_test)\\n\\nprint(\"loss: \", scores[0])\\nprint(\"ac: \", scores[1])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (!!!error)setup letnet-5 (7 layers)\n",
    "# alias avg_pooling == ap\n",
    "# conv1 + ap_1(5*5) + cov2 + ap_2(5*5) + full_1 + full_2 + output\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, (5,5), padding='same', input_shape=(28,28,1))) #1\n",
    "print(\"conv2d_1--> \", model.output_shape)\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))#2\n",
    "print(\"avg_pooling_1--> \", model.output_shape)\n",
    "model.add(Conv2D(16, (5,5)))#3\n",
    "print(\"conv2d_2--> \", model.output_shape)\n",
    "model.add(Dropout(0.375))\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))#4\n",
    "print(\"avg_pooling_2--> \", model.output_shape)\n",
    "model.add(Flatten())\n",
    "print(\"flatten --> \", model.output_shape)\n",
    "model.add(Dense(120))#5\n",
    "print(\"full_1 --> \", model.output_shape)\n",
    "model.add(Dense(84))#6\n",
    "print(\"full_2 --> \", model.output_shape)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print('output --> ', model.output_shape)#7\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "# train\n",
    "import datetime\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "# train\n",
    "start_tm = datetime.datetime.now()\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "end_tm = datetime.datetime.now()\n",
    "print(\"cost: \", (end_tm-start_tm).seconds)\n",
    "\n",
    "# evaluate\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"loss: \", scores[0])\n",
    "print(\"ac: \", scores[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding-->  (None, 32, 32, 1)\n",
      "conv2d_1-->  (None, 28, 28, 6)\n",
      "avg_pooling_1-->  (None, 14, 14, 6)\n",
      "conv2d_2-->  (None, 10, 10, 16)\n",
      "avg_pooling_2-->  (None, 5, 5, 16)\n",
      "conv2d_3 -->  (None, 1, 1, 120)\n",
      "full_1 -->  (None, 84)\n",
      "output -->  (None, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 09:48:55.004034 140243120809792 deprecation.py:323] From /home/super-workstation/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 378us/step - loss: 0.4273 - acc: 0.8854\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.2517 - acc: 0.9267\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.2097 - acc: 0.9399\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1847 - acc: 0.9482\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.1680 - acc: 0.9515\n",
      "cost:  42\n",
      "10000/10000 [==============================] - 0s 31us/step\n",
      "loss:  0.15955320706665516\n",
      "ac:  0.9551\n"
     ]
    }
   ],
   "source": [
    "######## Lenet5 1998 #########\n",
    "# (correction)setup lenet-5 (7 layers) 原始版本\n",
    "# alias avg_pooling == ap\n",
    "# conv1 + ap_1(5*5) + cov2 + ap_2(5*5) + conv_3 + full_1 + output\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, Dropout, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, units, gamma, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu = self.add_weight(name='mu',\n",
    "                                  shape=(int(input_shape[1]), self.units),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        diff = K.expand_dims(inputs) - self.mu\n",
    "        l2 = K.sum(K.pow(diff,2), axis=1) # 高斯径向基函数\n",
    "        res = K.exp(-1 * self.gamma * l2) # \n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D(2, input_shape=(28,28,1)))\n",
    "print(\"padding--> \", model.output_shape) \n",
    "#1 28×28×6\n",
    "# 训练参数计算规则 （cx_size*cy_size*上一层维度（特征图个数）+1）*当前层维度\n",
    "# 连接数 当前层和上一层的连接数  训练参数个数*特征图w*特征图h\n",
    "# 训练参数 156 = （5×5+1）×6 \n",
    "# 连接数 122304 = （5×5+1）×6×（28×28）\n",
    "model.add(Conv2D(6, (5,5)))#1 \n",
    "print(\"conv2d_1--> \", model.output_shape) \n",
    "#2 14×14×6 \n",
    "#训练参数 12 6*(1+1) 当前层数（偏置+采样参数）\n",
    "#连接数 5880 = （2×2 + 1）*6*(14*14)\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))#2\n",
    "print(\"avg_pooling_1--> \", model.output_shape)\n",
    "#3 10×10×16 \n",
    "#训练参数 1516 = (3*5*5+ 1)*6 + (4*5*5+1)*6 + (4*5*5+1)*3 + (6*5*5+1)\n",
    "#连接数 151600 = (3*5*5+ 1)*6×（10×10） + (4*5*5+1)*6×（10×10） + (4*5*5+1)*3*(10*10) + (6*5*5+1)*(10*10)\n",
    "model.add(Conv2D(16, (5,5)))#3\n",
    "print(\"conv2d_2--> \", model.output_shape)\n",
    "#4 5*5*16\n",
    "#训练参数 32 = 16*(1+1)\n",
    "#连接个数 2000 = （2*2*1 + 1）*6*(5*5) \n",
    "model.add(AveragePooling2D(pool_size=(2,2)))#4\n",
    "print(\"avg_pooling_2--> \", model.output_shape)\n",
    "#5 1*1*120\n",
    "#训练参数 48120 = (5*5*16 + 1)*120\n",
    "#连接个数 48120 = (5*5*16 + 1)*120*(1*1)\n",
    "model.add(Conv2D(120, (5,5)))#5\n",
    "print(\"conv2d_3 --> \", model.output_shape)\n",
    "model.add(Flatten())\n",
    "#6 1*1*84\n",
    "#训练参数 10164 = (1*1*120+1)*84\n",
    "#连接个数 10164 = (1*1*120+1)*84*(1*1)\n",
    "model.add(Dense(84, activation='tanh'))#6\n",
    "print(\"full_1 --> \", model.output_shape)\n",
    "#7 1*1*10\n",
    "#训练参数 850 = （1*1*84+1)*10(1*1)\n",
    "#连接个数 850 = （1*1*84+1)*10(1*1)\n",
    "#model.add(Dense(10, activation='sigmoid'))#7\n",
    "model.add(RBFLayer(10,0.5))\n",
    "print('output --> ', model.output_shape)\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "import datetime\n",
    "train_data = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "test_data = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "train_label = y_train\n",
    "test_label = y_test\n",
    "\n",
    "# train\n",
    "start_tm = datetime.datetime.now()\n",
    "history = model.fit(train_data, train_label, epochs=5)\n",
    "end_tm = datetime.datetime.now()\n",
    "print(\"cost: \", (end_tm-start_tm).seconds)\n",
    "\n",
    "scores = model.evaluate(test_data, test_label)\n",
    "\n",
    "print(\"loss: \", scores[0])\n",
    "print(\"ac: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 227, 227, 3)   (20000,)   (2000, 227, 227, 3)   (2000,)\n"
     ]
    }
   ],
   "source": [
    "########## Alexnet 2012 ###################\n",
    "# 数据预处理\n",
    "from dpl import utils\n",
    "\n",
    "dims = 227\n",
    "train_nums = 20000\n",
    "test_nums = 2000\n",
    "\n",
    "# 首次处理数据 将mnist数据集转换成需要形式（227×227×3）\n",
    "# x_train, y_train x_test y_test mnist 原始数据集\n",
    "'''\n",
    "import tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data(\"/home/super-workstation/program/LearnDeepLearning/data/mnist.npz\")\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "'''\n",
    "\n",
    "alex_train_label = y_train[:train_nums]\n",
    "alex_test_label = y_test[:test_nums]\n",
    "\n",
    "alex_train_data = utils.preprocess4Alexnet(x_train, train_nums, dsize=(dims, dims))\n",
    "alex_test_data = utils.preprocess4Alexnet(x_test, test_nums, dsize=(dims, dims))\n",
    "\n",
    "from dpl.alexnet import data_io\n",
    "data_io.save_data(\"../data/alex_mnist_data_\" + str(dims) + \".npz\", \n",
    "                 alex_train_data, alex_train_label,\n",
    "                 alex_test_data, alex_test_label)\n",
    "\n",
    "\n",
    "\n",
    "from dpl.alexnet import data_io\n",
    "(train_data, train_label),(test_data, test_label) = data_io.load_data(\"../data/alex_mnist_data_\" + str(dims) + \".npz\")\n",
    "\n",
    "print(train_data.shape, \" \", train_label.shape, \" \", test_data.shape, \" \", test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1（input）:  (None, 55, 55, 96)\n",
      "max_pool_1:  (None, 27, 27, 96)\n",
      "conv_2:  (None, 27, 27, 256)\n",
      "max_pool_2:  (None, 13, 13, 256)\n",
      "conv_3:  (None, 13, 13, 384)\n",
      "conv_4:  (None, 13, 13, 384)\n",
      "conv_5:  (None, 13, 13, 256)\n",
      "max_pool_3 (None, 6, 6, 256)\n",
      "full_1:  (None, 1, 1, 4096)\n",
      "flatten:  (None, 4096)\n",
      "full_2:  (None, 4096)\n",
      "full_3(output):  (None, 10)\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "   96/18000 [..............................] - ETA: 38:10 - loss: 2.6504 - acc: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super-workstation/.local/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.128000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 68s 4ms/step - loss: 0.5183 - acc: 0.8282 - val_loss: 0.0633 - val_acc: 0.9795\n",
      "Epoch 2/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.1101 - acc: 0.9673 - val_loss: 0.0498 - val_acc: 0.9850\n",
      "Epoch 3/12\n",
      "18000/18000 [==============================] - 56s 3ms/step - loss: 0.0711 - acc: 0.9790 - val_loss: 0.0421 - val_acc: 0.9855\n",
      "Epoch 4/12\n",
      "18000/18000 [==============================] - 57s 3ms/step - loss: 0.0527 - acc: 0.9855 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 5/12\n",
      "18000/18000 [==============================] - 57s 3ms/step - loss: 0.0407 - acc: 0.9873 - val_loss: 0.0351 - val_acc: 0.9905\n",
      "Epoch 6/12\n",
      "18000/18000 [==============================] - 57s 3ms/step - loss: 0.0319 - acc: 0.9904 - val_loss: 0.0320 - val_acc: 0.9880\n",
      "Epoch 7/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0249 - acc: 0.9922 - val_loss: 0.0266 - val_acc: 0.9915\n",
      "Epoch 8/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0248 - val_acc: 0.9925\n",
      "Epoch 9/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.0266 - val_acc: 0.9905\n",
      "Epoch 10/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0152 - acc: 0.9949 - val_loss: 0.0300 - val_acc: 0.9910\n",
      "Epoch 11/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0294 - val_acc: 0.9910\n",
      "Epoch 12/12\n",
      "18000/18000 [==============================] - 58s 3ms/step - loss: 0.0116 - acc: 0.9964 - val_loss: 0.0309 - val_acc: 0.9890\n",
      "2000/2000 [==============================] - 2s 1ms/step\n",
      "loss:  0.02026432618033141\n",
      "cost:  0.9935\n"
     ]
    }
   ],
   "source": [
    "########## Alexnet 2012 ###################\n",
    "# 8 layers 5 conv + 3 maxpool + 2 full + 1 output\n",
    "# 1-2 layers: conv(11*11, 5*5) + relu + normal + maxpool\n",
    "# 3-4 layers: conv(3*3, 3*3) + relu\n",
    "# 5 layers: conv + relu + maxpool\n",
    "# 6 layers: full(4096) + relu\n",
    "# 7 layers: full(4096)\n",
    "# 8 layers: full(1000 output)\n",
    "# 9 layers: full(10) 为mnist数据额外添加（因为mnist手写数据有10个类别）\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, BatchNormalization, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# 1 \n",
    "# conv1\n",
    "# output dim = (227-11)/4 + 1 = 55 --> 55*55*96\n",
    "model.add(Conv2D(96, (11,11), strides=4, activation='relu', input_shape=(227, 227, 3)))\n",
    "print(\"conv_1（input）: \", model.output_shape)\n",
    "# normalize layers\n",
    "model.add(BatchNormalization())\n",
    "# max_pool_1 27*27*96\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_1: \", model.output_shape)\n",
    "# 2\n",
    "# conv2 27*27*256\n",
    "model.add(Conv2D(256, (5,5), padding='same', activation='relu'))\n",
    "print(\"conv_2: \", model.output_shape)\n",
    "# normalize layers\n",
    "model.add(BatchNormalization())\n",
    "# max_pool_2 13*13*256\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_2: \", model.output_shape)\n",
    "# 3\n",
    "# conv3 13*13*384\n",
    "model.add(Conv2D(384, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_3: \", model.output_shape)\n",
    "# 4\n",
    "# conv4 13*13*384\n",
    "model.add(Conv2D(384, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_4: \", model.output_shape)\n",
    "# 5\n",
    "# conv5 13*13*256\n",
    "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_5: \", model.output_shape)\n",
    "# max_pool_3 6*6*256\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_3\", model.output_shape)\n",
    "\n",
    "# 6\n",
    "# full_1\n",
    "model.add(Conv2D(4096, (6,6), activation='relu'))\n",
    "print(\"full_1: \", model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "print(\"flatten: \", model.output_shape)\n",
    "# 7\n",
    "# full_2\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "print(\"full_2: \", model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "# 8\n",
    "# full_3\n",
    "#model.add(Dense(1000, activation='softmax'))\n",
    "# change for apply mnist\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))                \n",
    "model.add(Dense(10, activation='softmax'))  \n",
    "print(\"full_3(output): \", model.output_shape)\n",
    "\n",
    "# !使用adam 会导致结果不收敛\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history = model.fit(train_data, train_label, validation_split=0.1, epochs=12)\n",
    "\n",
    "# evaluate model\n",
    "scores = model.evaluate(test_data, test_label)\n",
    "print(\"loss: \", scores[0])\n",
    "print(\"cost: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "loss:  0.025560790936113336\n",
      "cost:  0.989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import cv2\n",
    "image_size = 227\n",
    "train_image = [cv2.cvtColor(cv2.resize(img,(image_size,image_size)),cv2.COLOR_GRAY2BGR) for img in x_train.astype('float32')]\n",
    "#test_image = [cv2.cvtColor(cv2.resize(img,(image_size,image_size)),cv2.COLOR_GRAY2BGR) for img in x_test]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1（input）:  (None, 111, 111, 96)\n",
      "max_pool_1:  (None, 55, 55, 96)\n",
      "conv_2:  (None, 28, 28, 256)\n",
      "max_pool_2:  (None, 13, 13, 256)\n",
      "conv_3:  (None, 13, 13, 384)\n",
      "conv_4:  (None, 13, 13, 384)\n",
      "conv_5:  (None, 13, 13, 256)\n",
      "max_pool_3 (None, 6, 6, 256)\n",
      "full_1:  (None, 1, 1, 4096)\n",
      "flatten:  (None, 4096)\n",
      "full_2:  (None, 4096)\n",
      "full_3(output):  (None, 10)\n"
     ]
    }
   ],
   "source": [
    "########## ZFnet 2013 ###################\n",
    "# 8 layers 5 conv + 3 maxpool + 2 full + 1 output\n",
    "# 1-2 layers: conv(7*7, 5*5) + relu + normal + maxpool\n",
    "# 3-4 layers: conv(3*3, 3*3) + relu\n",
    "# 5 layers: conv + relu + maxpool\n",
    "# 6 layers: full(4096) + relu\n",
    "# 7 layers: full(4096)\n",
    "# 8 layers: full(1000 output)\n",
    "# 9 layers: full(10) 为mnist数据额外添加（因为mnist手写数据有10个类别）\n",
    "# zfnet 和 alexnet 区别在于 1，2层:\n",
    "# alexnet 1 layer: conv 11*11*96 strides=4, 227*227*3 -> 55*55*96 + maxpool 55*55*96 -> 27*27*96\n",
    "#         2 layer: conv 5*5*256 strides=1, 27*27*96 -> 27*27*256 + maxpool 27*27*256 -> 13*13*256\n",
    "# zfnet   1 layer: conv 7*7*96 strides=2 227*227*3 -> 111*111*96 + maxpool 111*111*96 -> 55*55*96\n",
    "#         2 layer: conv 5*5*256 strides=2 padding=same 55*55*96 -> 27*27*256 + maxpool --> 13*13*256\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, BatchNormalization, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# 1 \n",
    "# conv1\n",
    "# output dim = (227-7)/2 + 1 = 111 --> 111*111*96\n",
    "model.add(Conv2D(96, (7,7), strides=2, activation='relu', input_shape=(227, 227, 3)))\n",
    "print(\"conv_1（input）: \", model.output_shape)\n",
    "# normalize layers\n",
    "model.add(BatchNormalization())\n",
    "# max_pool_1 55*55*96\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_1: \", model.output_shape)\n",
    "\n",
    "# 2\n",
    "# conv2 26*26*256\n",
    "model.add(Conv2D(256, (5,5), strides=2, padding = 'same', activation='relu'))\n",
    "print(\"conv_2: \", model.output_shape)\n",
    "# normalize layers\n",
    "model.add(BatchNormalization())\n",
    "# max_pool_2 13*13*256\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_2: \", model.output_shape)\n",
    "\n",
    "# 3\n",
    "# conv3 13*13*384\n",
    "model.add(Conv2D(384, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_3: \", model.output_shape)\n",
    "# 4\n",
    "# conv4 13*13*384\n",
    "model.add(Conv2D(384, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_4: \", model.output_shape)\n",
    "# 5\n",
    "# conv5 13*13*256\n",
    "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "print(\"conv_5: \", model.output_shape)\n",
    "# max_pool_3 6*6*256\n",
    "model.add(MaxPool2D(pool_size=(3,3), strides=2))\n",
    "print(\"max_pool_3\", model.output_shape)\n",
    "\n",
    "# 6\n",
    "# full_1\n",
    "model.add(Conv2D(4096, (6,6), activation='relu'))\n",
    "print(\"full_1: \", model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "print(\"flatten: \", model.output_shape)\n",
    "# 7\n",
    "# full_2\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "print(\"full_2: \", model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "# 8\n",
    "# full_3\n",
    "#model.add(Dense(1000, activation='softmax'))\n",
    "# change for apply mnist\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))                \n",
    "model.add(Dense(10, activation='softmax'))  \n",
    "print(\"full_3(output): \", model.output_shape)\n",
    "\n",
    "# !使用adam 会导致结果不收敛\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history = model.fit(train_data, train_label, validation_split=0.1, epochs=12)\n",
    "\n",
    "# evaluate model\n",
    "scores = model.evaluate(test_data, test_label)\n",
    "print(\"loss: \", scores[0])\n",
    "print(\"cost: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 224, 224, 3)   (20000,)   (2000, 224, 224, 3)   (2000,)\n"
     ]
    }
   ],
   "source": [
    "########## vggxnet 2014 ###################\n",
    "# 数据预处理\n",
    "from dpl import utils\n",
    "\n",
    "dims = 224\n",
    "train_nums = 20000\n",
    "test_nums = 2000\n",
    "\n",
    "# 首次处理数据 将mnist数据集转换成需要形式（227×227×3）\n",
    "# x_train, y_train x_test y_test mnist 原始数据集\n",
    "'''\n",
    "import tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data(\"/home/super-workstation/program/LearnDeepLearning/data/mnist.npz\")\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "'''\n",
    "\n",
    "alex_train_label = y_train[:train_nums]\n",
    "alex_test_label = y_test[:test_nums]\n",
    "\n",
    "alex_train_data = utils.preprocess4Alexnet(x_train, train_nums, dsize=(dims, dims))\n",
    "alex_test_data = utils.preprocess4Alexnet(x_test, test_nums, dsize=(dims, dims))\n",
    "\n",
    "from dpl.alexnet import data_io\n",
    "data_io.save_data(\"../data/alex_mnist_data_\" + str(dims) + \".npz\", \n",
    "                 alex_train_data, alex_train_label,\n",
    "                 alex_test_data, alex_test_label)\n",
    "\n",
    "\n",
    "\n",
    "from dpl.alexnet import data_io\n",
    "(train_data, train_label),(test_data, test_label) = data_io.load_data(\"../data/alex_mnist_data_\" + str(dims) + \".npz\")\n",
    "\n",
    "print(train_data.shape, \" \", train_label.shape, \" \", test_data.shape, \" \", test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1:  (None, 224, 224, 64)\n",
      "conv_2:  (None, 224, 224, 64)\n",
      "max_pool_1:  (None, 112, 112, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 18:11:33.281436 139987764008768 deprecation.py:506] From /home/super-workstation/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_3:  (None, 112, 112, 128)\n",
      "conv_4:  (None, 112, 112, 128)\n",
      "max_pool_2:  (None, 56, 56, 128)\n",
      "conv_5:  (None, 56, 56, 256)\n",
      "conv_6:  (None, 56, 56, 256)\n",
      "conv_7:  (None, 56, 56, 256)\n",
      "max_pool_3:  (None, 28, 28, 256)\n",
      "conv_8:  (None, 28, 28, 512)\n",
      "conv_9:  (None, 28, 28, 512)\n",
      "conv_10:  (None, 28, 28, 512)\n",
      "max_pool_4:  (None, 14, 14, 512)\n",
      "conv_11:  (None, 14, 14, 512)\n",
      "conv_12:  (None, 14, 14, 512)\n",
      "conv_13:  (None, 14, 14, 512)\n",
      "max_pool_5:  (None, 7, 7, 512)\n",
      "full_1:  (None, 4096)\n",
      "full_2:  (None, 4096)\n",
      "full_3:  (None, 1000)\n",
      "output:  (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 56, 56, 256)       65792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 28, 28, 512)       262656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 14, 14, 512)       262656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 133,648,962\n",
      "Trainable params: 133,648,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# vgg 16\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "# 64\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(224,224,3)))\n",
    "print(\"conv_1: \", model.output_shape)\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_2: \", model.output_shape)\n",
    "model.add(MaxPool2D())\n",
    "print(\"max_pool_1: \", model.output_shape)\n",
    "\n",
    "#128\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_3: \", model.output_shape)\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_4: \", model.output_shape)\n",
    "model.add(MaxPool2D())\n",
    "print(\"max_pool_2: \", model.output_shape)\n",
    "\n",
    "#256\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_5: \", model.output_shape)\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_6: \", model.output_shape)\n",
    "model.add(Conv2D(256, (1,1), activation='relu', padding='same'))\n",
    "print(\"conv_7: \", model.output_shape)\n",
    "model.add(MaxPool2D())\n",
    "print(\"max_pool_3: \", model.output_shape)\n",
    "\n",
    "#512\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_8: \", model.output_shape)\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_9: \", model.output_shape)\n",
    "model.add(Conv2D(512, (1,1), activation='relu', padding='same'))\n",
    "print(\"conv_10: \", model.output_shape)\n",
    "model.add(MaxPool2D())\n",
    "print(\"max_pool_4: \", model.output_shape)\n",
    "\n",
    "#512\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_11: \", model.output_shape)\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "print(\"conv_12: \", model.output_shape)\n",
    "model.add(Conv2D(512, (1,1), activation='relu', padding='same'))\n",
    "print(\"conv_13: \", model.output_shape)\n",
    "model.add(MaxPool2D())\n",
    "print(\"max_pool_5: \", model.output_shape)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "print(\"full_1: \", model.output_shape)\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "print(\"full_2: \", model.output_shape)\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "print(\"full_3: \", model.output_shape)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print(\"output: \", model.output_shape)\n",
    "\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 1.9738 - acc: 0.2918\n",
      "Epoch 2/12\n",
      "20000/20000 [==============================] - 139s 7ms/step - loss: 0.3118 - acc: 0.9038\n",
      "Epoch 3/12\n",
      "20000/20000 [==============================] - 139s 7ms/step - loss: 0.1500 - acc: 0.9541\n",
      "Epoch 4/12\n",
      "20000/20000 [==============================] - 140s 7ms/step - loss: 0.1074 - acc: 0.9657\n",
      "Epoch 5/12\n",
      "20000/20000 [==============================] - 138s 7ms/step - loss: 0.0859 - acc: 0.9742\n",
      "Epoch 6/12\n",
      "20000/20000 [==============================] - 139s 7ms/step - loss: 0.0712 - acc: 0.9781\n",
      "Epoch 7/12\n",
      "20000/20000 [==============================] - 141s 7ms/step - loss: 0.0605 - acc: 0.9819\n",
      "Epoch 8/12\n",
      "20000/20000 [==============================] - 140s 7ms/step - loss: 0.0519 - acc: 0.9844\n",
      "Epoch 9/12\n",
      "20000/20000 [==============================] - 141s 7ms/step - loss: 0.0432 - acc: 0.9864\n",
      "Epoch 10/12\n",
      "20000/20000 [==============================] - 142s 7ms/step - loss: 0.0392 - acc: 0.9888\n",
      "Epoch 11/12\n",
      "20000/20000 [==============================] - 139s 7ms/step - loss: 0.0337 - acc: 0.9891\n",
      "Epoch 12/12\n",
      "20000/20000 [==============================] - 141s 7ms/step - loss: 0.0299 - acc: 0.9905\n",
      "2000/2000 [==============================] - 6s 3ms/step\n",
      "loss:  0.03546125781838782\n",
      "acc:  0.988\n"
     ]
    }
   ],
   "source": [
    "def trainAndEvaluateData(model, train_data, train_label, test_data, test_label):\n",
    "    history = model.fit(train_data, train_label, epochs=12)\n",
    "\n",
    "    scores = model.evaluate(test_data, test_label)\n",
    "    print(\"loss: \", scores[0])\n",
    "    print(\"acc: \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 224, 224, 3)   (20000,)   (2000, 224, 224, 3)   (2000,)\n"
     ]
    }
   ],
   "source": [
    "########## googLenet 2014 ###################\n",
    "# 数据预处理\n",
    "from dpl import utils\n",
    "\n",
    "dims = 224\n",
    "train_nums = 20000\n",
    "test_nums = 2000\n",
    "\n",
    "from dpl.alexnet import data_io\n",
    "(train_data, train_label),(test_data, test_label) = data_io.load_data(\"../data/alex_mnist_data_\" + str(dims) + \".npz\")\n",
    "\n",
    "print(train_data.shape, \" \", train_label.shape, \" \", test_data.shape, \" \", test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 112, 112, 64) 9472        input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_152 (MaxPooling2D (None, 56, 56, 64)   0           conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 56, 56, 64)   256         max_pooling2d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 56, 56, 64)   4160        batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 56, 56, 192)  110784      conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 56, 56, 192)  768         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_153 (MaxPooling2D (None, 28, 28, 192)  0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 28, 28, 96)   18528       max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 28, 28, 16)   3088        max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 28, 28, 96)   384         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 28, 28, 16)   64          conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 28, 28, 96)   0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 28, 28, 16)   0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_154 (MaxPooling2D (None, 28, 28, 192)  0           max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 28, 28, 64)   12352       max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 28, 28, 128)  110720      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 28, 28, 32)   12832       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 28, 28, 32)   6176        max_pooling2d_154[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 28, 28, 64)   256         conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 28, 28, 128)  512         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 28, 28, 32)   128         conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 28, 28, 32)   128         conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 28, 28, 64)   0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 28, 28, 128)  0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 28, 28, 32)   0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 28, 28, 32)   0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 28, 28, 256)  0           activation_169[0][0]             \n",
      "                                                                 activation_171[0][0]             \n",
      "                                                                 activation_173[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 28, 28, 128)  32896       concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 28, 28, 32)   8224        concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 28, 28, 128)  512         conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 28, 28, 32)   128         conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 28, 28, 128)  0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 28, 28, 32)   0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_155 (MaxPooling2D (None, 28, 28, 256)  0           concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 28, 28, 128)  32896       concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 28, 28, 192)  221376      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 28, 28, 96)   76896       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 28, 28, 64)   16448       max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 28, 28, 128)  512         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 28, 28, 192)  768         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 28, 28, 96)   384         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 28, 28, 64)   256         conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 28, 28, 128)  0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 28, 28, 192)  0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 28, 28, 96)   0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 28, 28, 64)   0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 28, 28, 480)  0           activation_175[0][0]             \n",
      "                                                                 activation_177[0][0]             \n",
      "                                                                 activation_179[0][0]             \n",
      "                                                                 activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_156 (MaxPooling2D (None, 14, 14, 480)  0           concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 14, 14, 96)   46176       max_pooling2d_156[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 14, 14, 16)   7696        max_pooling2d_156[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 14, 14, 96)   384         conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 14, 14, 16)   64          conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 14, 14, 96)   0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 14, 14, 16)   0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_157 (MaxPooling2D (None, 14, 14, 480)  0           max_pooling2d_156[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 14, 14, 192)  92352       max_pooling2d_156[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 14, 14, 208)  179920      activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 14, 14, 48)   19248       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 14, 14, 64)   30784       max_pooling2d_157[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 14, 14, 192)  768         conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 14, 14, 208)  832         conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 14, 14, 48)   192         conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 14, 14, 64)   256         conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 14, 14, 192)  0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 14, 14, 208)  0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 14, 14, 48)   0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 14, 14, 64)   0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 14, 14, 512)  0           activation_181[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "                                                                 activation_185[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 14, 14, 112)  57456       concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 14, 14, 24)   12312       concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 14, 14, 112)  448         conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 14, 14, 24)   96          conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 14, 14, 112)  0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 14, 14, 24)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_158 (MaxPooling2D (None, 14, 14, 512)  0           concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 14, 14, 160)  82080       concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 14, 14, 224)  226016      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 14, 14, 64)   38464       activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 14, 14, 64)   32832       max_pooling2d_158[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 14, 14, 160)  640         conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 14, 14, 224)  896         conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 14, 14, 64)   256         conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 14, 14, 64)   256         conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 14, 14, 160)  0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 14, 14, 224)  0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 14, 14, 64)   0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 14, 14, 64)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 14, 14, 512)  0           activation_188[0][0]             \n",
      "                                                                 activation_190[0][0]             \n",
      "                                                                 activation_192[0][0]             \n",
      "                                                                 activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 14, 14, 128)  65664       concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 14, 14, 24)   12312       concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 14, 14, 128)  512         conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 14, 14, 24)   96          conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 14, 14, 128)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 14, 14, 24)   0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_159 (MaxPooling2D (None, 14, 14, 512)  0           concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 14, 14, 128)  65664       concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 14, 14, 256)  295168      activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 14, 14, 64)   38464       activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 14, 14, 64)   32832       max_pooling2d_159[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 14, 14, 128)  512         conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 14, 14, 256)  1024        conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 14, 14, 64)   256         conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 14, 14, 64)   256         conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 14, 14, 128)  0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 14, 14, 256)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 14, 14, 64)   0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 14, 14, 64)   0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 14, 14, 512)  0           activation_194[0][0]             \n",
      "                                                                 activation_196[0][0]             \n",
      "                                                                 activation_198[0][0]             \n",
      "                                                                 activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 14, 14, 114)  58482       concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 14, 14, 32)   16416       concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 14, 14, 114)  456         conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 14, 14, 32)   128         conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 14, 14, 114)  0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 14, 14, 32)   0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_160 (MaxPooling2D (None, 14, 14, 512)  0           concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 14, 14, 112)  57456       concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 14, 14, 288)  295776      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 14, 14, 64)   51264       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 14, 14, 64)   32832       max_pooling2d_160[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 14, 14, 112)  448         conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 14, 14, 288)  1152        conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 14, 14, 64)   256         conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 14, 14, 64)   256         conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 14, 14, 112)  0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 14, 14, 288)  0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 14, 14, 64)   0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 14, 14, 64)   0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 14, 14, 528)  0           activation_200[0][0]             \n",
      "                                                                 activation_202[0][0]             \n",
      "                                                                 activation_204[0][0]             \n",
      "                                                                 activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 14, 14, 160)  84640       concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 14, 14, 32)   16928       concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 14, 14, 160)  640         conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 14, 14, 32)   128         conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 14, 14, 160)  0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 14, 14, 32)   0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_161 (MaxPooling2D (None, 14, 14, 528)  0           concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 14, 14, 256)  135424      concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 14, 14, 320)  461120      activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 14, 14, 128)  102528      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 14, 14, 128)  67712       max_pooling2d_161[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 14, 14, 256)  1024        conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 14, 14, 320)  1280        conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 14, 14, 128)  512         conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 14, 14, 128)  512         conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 14, 14, 256)  0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 14, 14, 320)  0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 14, 14, 128)  0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 14, 14, 128)  0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 14, 14, 832)  0           activation_207[0][0]             \n",
      "                                                                 activation_209[0][0]             \n",
      "                                                                 activation_211[0][0]             \n",
      "                                                                 activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_162 (MaxPooling2D (None, 7, 7, 832)    0           concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 7, 7, 160)    133280      max_pooling2d_162[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 7, 7, 32)     26656       max_pooling2d_162[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 7, 7, 160)    640         conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 7, 7, 32)     128         conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 7, 7, 160)    0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 7, 7, 32)     0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_163 (MaxPooling2D (None, 7, 7, 832)    0           max_pooling2d_162[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 7, 7, 256)    213248      max_pooling2d_162[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 7, 7, 320)    461120      activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 7, 7, 128)    102528      activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 7, 7, 128)    106624      max_pooling2d_163[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 7, 7, 256)    1024        conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 7, 7, 320)    1280        conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 7, 7, 128)    512         conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 7, 7, 128)    512         conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 7, 7, 256)    0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 7, 7, 320)    0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 7, 7, 128)    0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 7, 7, 128)    0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 7, 7, 832)    0           activation_213[0][0]             \n",
      "                                                                 activation_215[0][0]             \n",
      "                                                                 activation_217[0][0]             \n",
      "                                                                 activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 7, 7, 192)    159936      concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 7, 7, 48)     39984       concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 7, 7, 192)    768         conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 7, 7, 48)     192         conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 7, 7, 192)    0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 7, 7, 48)     0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_164 (MaxPooling2D (None, 7, 7, 832)    0           concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 7, 7, 384)    319872      concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 7, 7, 384)    663936      activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 7, 7, 128)    153728      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 7, 7, 128)    106624      max_pooling2d_164[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 7, 7, 384)    1536        conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 7, 7, 384)    1536        conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 7, 7, 128)    512         conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 7, 7, 128)    512         conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 7, 7, 384)    0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 7, 7, 384)    0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 7, 7, 128)    0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 7, 7, 128)    0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 7, 7, 1024)   0           activation_219[0][0]             \n",
      "                                                                 activation_221[0][0]             \n",
      "                                                                 activation_223[0][0]             \n",
      "                                                                 activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_36 (AveragePo (None, 1, 1, 1024)   0           concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 1, 1, 1024)   0           average_pooling2d_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 1024)         0           dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 1000)         1025000     flatten_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1000)         0           dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 10)           10010       dropout_63[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,944,156\n",
      "Trainable params: 6,929,784\n",
      "Non-trainable params: 14,372\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#google net 2014\n",
    "# 训练时需要副分类器， 预测时无视\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, AveragePooling2D, Dense, Flatten\n",
    "from keras.layers import BatchNormalization, Input, Dropout, Activation\n",
    "\n",
    "def conv2d_bn(input, nums_kernal, size, strides=1, padding = 'same'):\n",
    "    x = Conv2D(nums_kernal, size, padding=padding, strides=strides)(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def getMaxPool(input):\n",
    "    return MaxPooling2D(pool_size=(3,3), strides=1, padding='same')(input)\n",
    "\n",
    "# conv strides=1 padding=same\n",
    "# maxpool size(3,3) strides=2 padding='same'\n",
    "# nums --> nums of filter for special size(1x1, 3x3, 5x5, pool)\n",
    "def Inception(input, nums_11, nums_11_33, nums_33, nums_11_55, nums_55, nums_11_pool):\n",
    "    conv_11 = conv2d_bn(input, nums_11, (1,1))\n",
    "    \n",
    "    conv_11_33 = conv2d_bn(input, nums_11_33, (1,1))\n",
    "    conv_33 = conv2d_bn(conv_11_33, nums_33, (3,3))\n",
    "    \n",
    "    conv_11_55 = conv2d_bn(input, nums_11_55, (1,1))\n",
    "    conv_55 = conv2d_bn(conv_11_55, nums_55, (5,5))\n",
    "    \n",
    "    conv_max_pool = getMaxPool(input)\n",
    "    conv_max_pool_11 = conv2d_bn(conv_max_pool, nums_11_pool, (1,1))\n",
    "    \n",
    "    output = keras.layers.concatenate([conv_11, conv_33, conv_55, conv_max_pool_11], axis=-1)\n",
    "    return output\n",
    "\n",
    "def AuxiliaryClassifier(input):\n",
    "    x = AveragePooling2D(pool_size=(5, 5), strides=3)(input)\n",
    "    x = conv2d_bn(x, 128, (1,1))\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    #output = Dense(1000, activation='softmax')(x)\n",
    "    # add 1 full layer for mnist\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    output = Dense(10, activation = 'softmax')(x)\n",
    "    return output\n",
    "\n",
    "def MainClassifier(input):\n",
    "    x = AveragePooling2D(pool_size=(7, 7), strides=1)(input)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    #output = Dense(1000, activation='softmax')(x)\n",
    "    # add 1 full layer for mnist\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    return output\n",
    "    \n",
    "    \n",
    "input = Input(shape=(224, 224, 3))\n",
    "x = Conv2D(64, (7,7), strides=2, padding='same', activation='relu')(input)\n",
    "#x = conv2d_bn(input, 64, size=(7,7), strides=2)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(64, (1,1), activation='relu')(x)\n",
    "x = Conv2D(192, (3,3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
    "\n",
    "# inception_3a\n",
    "x = Inception(x, 64, 96, 128, 16, 32, 32)\n",
    "# inception_3b\n",
    "x = Inception(x, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
    "\n",
    "# inception_4a\n",
    "x = Inception(x, 192, 96, 208, 16, 48, 64)\n",
    "# Auxiliary Classifier 0\n",
    "softmax0 = AuxiliaryClassifier(x)\n",
    "# inception_4b\n",
    "x = Inception(x, 160, 112, 224, 24, 64, 64)\n",
    "# inception_4c\n",
    "x = Inception(x, 128, 128, 256, 24, 64, 64)\n",
    "# inception_4d\n",
    "x = Inception(x, 112, 114, 288, 32, 64, 64)\n",
    "# Auxiliary Classifier 1\n",
    "softmax1 = AuxiliaryClassifier(x)\n",
    "# inception_4e\n",
    "x = Inception(x, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(x)\n",
    "\n",
    "# inception_5a\n",
    "x = Inception(x, 256, 160, 320, 32, 128, 128)\n",
    "# inception_5b\n",
    "x = Inception(x, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "# Main classifier\n",
    "softmax2 = MainClassifier(x)\n",
    "\n",
    "#[softmax0, softmax1, softmax2]\n",
    "model = Model(input, outputs=softmax2, name=\"googLenet\")\n",
    "#model.build(input)\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.get_config()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "18000/18000 [==============================] - 57s 3ms/step - loss: 0.1535 - acc: 0.9584 - val_loss: 0.0540 - val_acc: 0.9815\n",
      "Epoch 2/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0715 - acc: 0.9806 - val_loss: 0.0545 - val_acc: 0.9845\n",
      "Epoch 3/12\n",
      "18000/18000 [==============================] - 56s 3ms/step - loss: 0.0547 - acc: 0.9844 - val_loss: 0.0599 - val_acc: 0.9820\n",
      "Epoch 4/12\n",
      "18000/18000 [==============================] - 54s 3ms/step - loss: 0.0401 - acc: 0.9883 - val_loss: 0.0693 - val_acc: 0.9795\n",
      "Epoch 5/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0314 - acc: 0.9911 - val_loss: 0.1001 - val_acc: 0.9705\n",
      "Epoch 6/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.1082 - val_acc: 0.9690\n",
      "Epoch 7/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0434 - val_acc: 0.9880\n",
      "Epoch 8/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0134 - acc: 0.9969 - val_loss: 0.0247 - val_acc: 0.9925\n",
      "Epoch 9/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0103 - acc: 0.9972 - val_loss: 0.0346 - val_acc: 0.9905\n",
      "Epoch 10/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0094 - acc: 0.9978 - val_loss: 0.0263 - val_acc: 0.9915\n",
      "Epoch 11/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0340 - val_acc: 0.9895\n",
      "Epoch 12/12\n",
      "18000/18000 [==============================] - 55s 3ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0440 - val_acc: 0.9880\n",
      "2000/2000 [==============================] - 3s 1ms/step\n",
      "loss:  0.06674277622526278\n",
      "acc:  0.9765\n"
     ]
    }
   ],
   "source": [
    "def trainAndEvaluateData(model, train_data, train_label, test_data, test_label):\n",
    "    history = model.fit(train_data, train_label, validation_split = 0.1, epochs=12)\n",
    "    scores = model.evaluate(test_data, test_label)\n",
    "    print(\"loss: \", scores[0])\n",
    "    print(\"acc: \", scores[1])\n",
    "\n",
    "#utils.showImages(train_data)\n",
    "trainAndEvaluateData(model, train_data, train_label, test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
